# -*- coding: utf-8 -*-
"""ANN_TensorFlow_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DEki0DRMqkO866dladwHCQfGNbwZVBc-
"""

#Import needed libraries
import tensorflow as tf
from tensorflow.keras import datasets
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)   #show the version of TensorFlow we are using

#Load the MNIST dataset
mnist = datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

#Visualise a few examples
plt.figure(figsize=(10, 2))
for index, (image, label) in enumerate(zip(x_train[0:5], y_train[0:5])):
    plt.subplot(1, 5, index + 1)
    plt.imshow(np.reshape(image, (28, 28)), cmap=plt.cm.gray)
    plt.title('Training: {}\n'.format(label), fontsize = 10)

#Convert data from integers to floating-point numbers (between 0 and 1)
x_train, x_test = x_train / 255.0, x_test / 255.0

#Build the tf.keras.Sequential model by adding a layer after another 
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

#Compile the model : add optimizer, loss, and performance evaluation metrics
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy', #used when the true labels are integers - 'categorical_crossentropy' is used when the true labels are one-hot encoded 
              metrics=['accuracy'])

#Train
model.fit(x_train, y_train, epochs=5)

#Evaluate
loss, accuracy = model.evaluate(x_test, y_test)
print("loss = {}, accuracy = {}".format(loss, accuracy))

import seaborn as sns
from sklearn import metrics
predictions = model.predict(x_test)  #gives the probabilities of belonging of this image to the classes
print(predictions)

#To get the classes:
classes = np.argmax(predictions, axis = 1)  #finds the index of the highest value in each row, axis=1 means that the operation is across rows
print(classes)

#Visualise
plt.figure(figsize=(10, 2))
for index, (image, label) in enumerate(zip(x_test[0:5], classes[0:5])):
    plt.subplot(1, 5, index + 1)
    plt.imshow(np.reshape(image, (28, 28)), cmap=plt.cm.gray)
    plt.title('Training: {}\n'.format(label), fontsize = 10)

cm = metrics.confusion_matrix(y_test, classes, normalize='true')
plt.figure(figsize=(9,9))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
all_sample_title = 'Accuracy = {0}'.format(accuracy)
plt.title(all_sample_title, size = 15)

#Callbacks : A callback performs actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch)
#Early Stopping
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
import os, datetime

early_stopping = EarlyStopping(monitor='val_loss', patience=5)  #which quantity to monitor to decide early stopping
#patience = the number of epochs to wait before stopping

#Notice that since we set patience=5, we wonâ€™t get the best model, but the model two epochs after the best model.
#==> An additional callback is required that saves the best model observed during training. This is the ModelCheckpoint callback

logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

callbacks = [early_stopping, ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True),
             TensorBoard(logdir, histogram_freq=1)]

history = model.fit(
    x_train, 
    y_train, 
    epochs=50, 
    validation_split=0.25, 
    batch_size=40, 
    verbose=2,
    callbacks=[callbacks]
)


#The saved model can then be loaded and evaluated any time by calling the load_model() function.
from keras.models import load_model
saved_model = load_model('best_model.h5')
test_acc = saved_model.evaluate(x_test, y_test)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

'''#Regularizers :
from tensorflow.keras.regularizers import l2, l1
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(4, activation='relu', kernel_regularizer=l1(l=0.1)))
model.add(tf.keras.layers.Dense(4, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=0.1, l2=0.01)))
'''