# -*- coding: utf-8 -*-
"""Binary_Classification_Bank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KAj6c4kCgK4M-MdmTV6JxWvDRJNlXohC

We need to train a model that predicts if a customer will leave the bank or not.
"""

import pandas as pd

dataset = pd.read_csv('Churn_Modelling.csv')  
#Kaggle database (source: https://www.kaggle.com/aakash50897/churn-modellingcsv?select=Churn_Modelling.csv)
dataset

print(dataset.columns)

print(dataset.index)

#Not all independent variables are important for the result (such as RowNumber, CustomerId)
X = dataset.iloc[:, 3: 13].values
X

y = dataset.iloc[:, 13].values
y

#Data encoding :
#We have to encode categorical data (such as Geography and Gender)
#ORDINAL ENCODING - way 1:
from sklearn.preprocessing import LabelEncoder

X_ord_1 = X

labelencoder_X_1 = LabelEncoder() #instantiate an object of the class LabelEncoder
X_ord_1[:, 1] = labelencoder_X_1.fit_transform(X[:, 1]) #ordinal encoding for column 1

labelencoder_X_2 = LabelEncoder()
X_ord_1[:, 2] = labelencoder_X_2.fit_transform(X[:, 2]) #ordinal encoding for column 2
X_ord_1

#ORDINAL ENCODING - way 2:
from sklearn.preprocessing import OrdinalEncoder

X_ord_2 = X

ordinal_encoder_1 = OrdinalEncoder()
X_ord_2[:, 1] = ordinal_encoder_1.fit_transform([X[:, 1]])
X_ord_2

X = X_ord_1

#ONE-HOT ENCODING :
#Way 1 : using data values :
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
import numpy as np
ct = ColumnTransformer( #'encoder' is the name of the column transformer
    [('encoder', OneHotEncoder(), [1])],    # The column numbers to be transformed (here is [1] but can be [0, 1, 3])
    remainder='passthrough'                         # Leave the rest of the columns untouched
)
X = np.array(ct.fit_transform(X), dtype=np.float)
df = pd.DataFrame(X)
df

#ONE-HOT ENCODING :
#Way 2 : using data frame :
X_df = dataset.iloc[:, 3: 13]
X_df = pd.concat([X_df, pd.get_dummies(X_df['Geography'], prefix='country', drop_first=True)], axis=1)  #drops the first column
#axis = 1 means to concatenate along the columns (put one column beside another)
X_df.drop(['Geography'], axis=1, inplace=True)  #get rid of the original Geography column
X_df

#We remove the first column to avoid the dummy data trap
'''Dummy data trap : A scenario where independent variables are highly correlated (one variable predicts the value of others). 
In one-hot encoding, one dummy variable can be predicted through other dummy variables, thus causing redundancy
==> Using all dummy variables for regression models lead to dummy variable trap
==> We exclude one of those dummy variable.'''

X = X[:, 1:]
df = pd.DataFrame(X)
df

# Split the data into training and test set (20% for the test set)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2, random_state = 10) # We use random_state to make sure splitting contains the same data each time.

#Standardise the data (x_standardised = (x - x_mean)/std_dev)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test) #note that we use the scale set from the training set to transform the test set
df = pd.DataFrame(X_train)
df

#Building the model : 
#We use 2 dense layers

import tensorflow as tf
model = tf.keras.models.Sequential()

#add input layer and first hidden layer
model.add(tf.keras.layers.Dense(units=6, kernel_initializer='uniform', activation='relu'))

#add 2nd hidden layer
model.add(tf.keras.layers.Dense(units=6, kernel_initializer='uniform', activation='relu'))

'''The number of units is mainly chosen by experience. In general, based on experimentation, 
setting it to the average between the number of the input nodes (11) and the number of the 
ouput nodes (1). Cross-validation can also be used to choose to choose the best parameters (parameter tuning).'''

'''random_uniform: Weights are initialized to uniformly random small values between -0.05 to 0.05.
random_normal: Weights are initialized according to a Gaussian distribution, with zero mean and a small standard deviation of 0.05.
zero: All weights are initialized to zero.'''

#Add the output layer
model.add(tf.keras.layers.Dense(units=1, kernel_initializer='uniform', activation='sigmoid')) #Sigmoid for binary, Softmax for multiclass

#Compilation
model.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = ['accuracy'])

#Training
history = model.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose=0)

loss, accuracy = model.evaluate(X_test, y_test)

#Evaluation
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5)
print(y_pred)

#Predict using the info of a new customer
new_customer = [[0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]]
new_customer = sc.transform(new_customer)
new_prediction = model.predict(new_customer)
new_prediction = (new_prediction > 0.5)
print(new_prediction)

from matplotlib import pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['loss'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.show()

#Note : An alternative to using train_test_split() is to specify a validation_split percentage. 
#This is done when fitting the model, for example:

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)

history = model.fit(X,y, verbose = 0,
                    validation_split = 0.2, # split data in 80-20 sets
                    epochs = 100,
                    batch_size = 10)

from matplotlib import pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['accuracy','validation accuracy', 'loss', 'validation_loss'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.show()